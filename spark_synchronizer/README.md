# Что нужно сделать новому сотруднику в рекомендашках чтобы ~~жизнь стала лучше~~ Spark автоматически собирался:

> :information_source: Всё пытаемся делать под рутом

0. Ставим ncurses-dialog, если ещё не:
```bash
apt-get install dialog
```

1. Клоним к себе данную репу:
```bash
git clone https://github.com/E7su/geodude.git
```

2. Запускаем spark_deb_builder.sh:
```bash
spark-deb-builder/spark_deb_builder.sh
```

3. В кнопочном интерфейсе выбираем версию v2.2.0, blas default и конфиги local. 
Для тех, кому интересно почему именно так и что это меняет, ссылочка на доку по [Spark-deb-builder'у](https://github.com/E7su/geodude/tree/master/spark_deb_builder)
4. Ждём когда соберётся

5. Заходим в папочку spark2 и ставим новый спарк:
```bash
dpkg -i spark....
```
6. Заходим в папочку spark-synchronizer, она будет где-то тут:
```bash
cd /usr/lib/spark2/spark-synchronizer
``` 
7. Запускаем preparator.sh. Он добавит необходимые переменные окружения и пропишет крон:
```bash
./preparator.sh
```
После первого запуска preparator.sh нужно вручную запустить rebuilder чтобы файлы конфигов попали в правильные места :)  
Будет пофикшено в следующей версии
```bash
cd /usr/lib/spark2/spark-synchronizer/
./rebuilder.sh
```

## =============================================
Теперь при любых изменениях конфигов на хосте, спарк будет автоматически пересобираться и писать об этом в консоль root. Письма можно прочитать тут:
```bash
cat /var/mail/root ./conf_pusher.sh
```
> :information_source: Письмо со словом rebuild означает, что кто-то изменил конфиг на сервере и новый спарк начал собираться.

information_source: Посмотреть как всё пересобирается можно тут:
```bash
tail -f /usr/lib/spark2/spark-synchronizer/build_log
```
Вот эти слова говорят о том что всё прошло в штатном режиме и можно устанавливать собранный пакет
```
Finished running lintian.
<<< Сборка deb-пакета завершена
>>> Копирование hive-site.xml
cp conf/hive-site.xml /
<<< Копирование hive-site.xml завершено
>>> Копирование yarn-site.xml и core-site.xml
cp conf/yarn-site.xml /
cp conf/core-site.xml /
<<< Копирование yarn-site.xml и core-site.xml завершено
```


Дальше нужно будет только заходить в папочку spark2, находящуюся тут и устанавливать новый спарк по мере необходимости:
```bash
cd /usr/lib/spark2/spark-deb-builder/spark2/
dpkg -i spark....
```


:warning: :warning: :warning: 
# Инструкция (для тех кто решил обновить конфигурацию спарка на сервере предпрода):

0. Заходим на ноду:
```bash
ssh ...
```
1. Заходим сюда:
```bash
cd /usr/bin/spark-synchronizer
```
2. Открываем в нашем любимом редакторе файлик, который собрались редактировать (редактировать можно только конфиги спарка!!1!11!)

> :warning:  Конфиги yarn, hadoop, hive и прочее можно редактировать только из cloudera-manager, ~~то есть нельзя редактировать~~ 
```bash
nano /usr/lib/spark2/conf/spark-defaults.conf
vim /usr/lib/spark2/conf/spark-defaults.conf
```

3. После того как поменяли - запускаем скрипт для коммита и пуша данных изменений в гит. Запускать от пользователя da
Выбираем нужные файлы точечками мышкой. Если два раза тыкнуть на точечку, то выбор данного файла снимется ~~что логично~~
```bash
./conf_pusher.sh 
```

4. После данных изменений автоматически у всех кто запускал когда-то preparator.sh ~~должен пересобраться~~ пересоберётся Spark


Под капотом:
По сути своей, синхронизирующий механизм представляет из себя pub/sub систему, шиной которой является git, pub - сервер, а sub - локальные машины, желающие получить наиболее актуальный пакет Spark.
Были различные гипотезы построения observer и других pub/sub систем с использованием docker, ansible, puppet, salt. Но решили остановится именно на системе, использующей git для хранения конфигов и сбориающей deb-пакеты, которые легко можно установить (и для которых уже был почти готовые механизм сборки)

Состоит из 4х частей:
- spark_deb_builder.sh - собственно сам сборщик деб пакетов (который мы уже использовали ранее и который был чуточку обогащён новым функционалом)
- conf_pusher.sh - пушер конфигов с сервера (после изменения конфигов, можно вызвать данный скрипт, поставить галочки на измененные файлы и нажать ок, скрипт сам их запушит)
- preparator.sh - нужен для того, чтобы настроить локальную машину data scientist'a, создав там необходимые переменные окружения и настроив крон на автоматическую сборку deb-пакетов в случае расхождения файлов конфигурации между локальной машиной и сервером
- rebuilder.sh - штука, которая как раз автоматически пересобирает спарк
